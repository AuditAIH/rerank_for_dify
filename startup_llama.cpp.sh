#!/bin/bash
set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡ºï¼Œä¿è¯è„šæœ¬å¥å£®æ€§

# ====================== ç¬¬ä¸€æ­¥ï¼šå®šä¹‰æ ¸å¿ƒå˜é‡ï¼ˆæ–¹ä¾¿åŽç»­ç»´æŠ¤ï¼‰ ======================
echo -e "\033[34mã€æ­¥éª¤1/10ã€‘åˆå§‹åŒ–æ ¸å¿ƒé…ç½®å˜é‡...\033[0m"
# CUDA 13å®˜æ–¹é»˜è®¤åŠ¨æ€é“¾æŽ¥åº“è·¯å¾„ï¼ˆ64ä½ç³»ç»Ÿï¼‰
CUDA13_LIB_PATH="/usr/local/cuda-13/lib64"
# ollamaè‡ªå¸¦çš„CUDA 13åŠ¨æ€é“¾æŽ¥åº“è·¯å¾„
OLLAMA_CUDA_PATH="/usr/local/lib/ollama/cuda_v13"
# åŽŸå§‹GitHubé¢„ç¼–è¯‘åŒ…ä¸‹è½½åœ°å€
LLAMA_DOWNLOAD_URL="https://github.com/AuditAIH/llama.cpp_rerank/releases/download/0.01/llama.cpp_rerank.tar.gz"
# GitHubä»£ç†å‰ç¼€
GH_PROXY_PREFIX="https://gh-proxy.org/"
# é‡æŽ’åºæ¨¡åž‹ä¸‹è½½åœ°å€
MODEL_DOWNLOAD_URL="https://www.modelscope.cn/models/ggml-org/Qwen3-Reranker-0.6B-Q8_0-GGUF/resolve/master/qwen3-reranker-0.6b-q8_0.gguf"
# å·¥ä½œç›®å½•ï¼ˆllama.cpp_rerankçš„æ ¹ç›®å½•ï¼‰
LLAMA_ROOT_DIR="$PWD/llama.cpp_rerank"
# å¯åŠ¨è„šæœ¬è·¯å¾„
START_SCRIPT_PATH="$PWD/start_llama.sh"
# systemdæœåŠ¡æ–‡ä»¶è·¯å¾„
SERVICE_FILE_PATH="/etc/systemd/system/llama-server.service"
# æ¨¡åž‹æ–‡ä»¶å®Œæ•´è·¯å¾„
MODEL_FILE_PATH="$LLAMA_ROOT_DIR/qwen3-reranker-0.6b-q8_0.gguf"

# ====================== ç¬¬äºŒæ­¥ï¼šæ£€æµ‹CUDAçŽ¯å¢ƒ ======================
echo -e "\033[34mã€æ­¥éª¤2/10ã€‘æ£€æµ‹CUDA 13åŠ¨æ€é“¾æŽ¥åº“...\033[0m"
# åˆå§‹åŒ–CUDAåº“è·¯å¾„å˜é‡
CUDA_LIB_DIR=""

# 1. æ£€æµ‹å®˜æ–¹CUDA 13
if [ -d "$CUDA13_LIB_PATH" ]; then
    echo -e "\033[32mâœ… æ£€æµ‹åˆ°å®˜æ–¹CUDA 13åº“ï¼š$CUDA13_LIB_PATH\033[0m"
    CUDA_LIB_DIR="$CUDA13_LIB_PATH"
# 2. æ£€æµ‹ollamaè‡ªå¸¦çš„CUDA 13
elif [ -d "$OLLAMA_CUDA_PATH" ]; then
    echo -e "\033[32mâœ… æ£€æµ‹åˆ°ollamaè‡ªå¸¦çš„CUDA 13åº“ï¼š$OLLAMA_CUDA_PATH\033[0m"
    CUDA_LIB_DIR="$OLLAMA_CUDA_PATH"
# 3. ä¸¤è€…éƒ½ä¸å­˜åœ¨ï¼Œæç¤ºä¸‹è½½
else
    echo -e "\033[31mâŒ æœªæ£€æµ‹åˆ°CUDA 13æˆ–ollamaè‡ªå¸¦çš„CUDA 13åº“ï¼\033[0m"
    echo -e "\033[33mè¯·å‰å¾€NVIDIAå®˜ç½‘ä¸‹è½½CUDA 13ï¼šhttps://developer.nvidia.com/cuda-13-0-0-download-archive\033[0m"
    exit 1  # é€€å‡ºè„šæœ¬ï¼Œé¿å…åŽç»­æ— æ•ˆæ“ä½œ
fi

# ====================== ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºå·¥ä½œç›®å½• ======================
echo -e "\033[34mã€æ­¥éª¤3/10ã€‘åˆ›å»ºllama.cpp_rerankå·¥ä½œç›®å½•...\033[0m"
mkdir -p "$LLAMA_ROOT_DIR"
echo -e "\033[32mâœ… ç›®å½•åˆ›å»ºæˆåŠŸï¼š$LLAMA_ROOT_DIR\033[0m"

# ====================== ç¬¬å››æ­¥ï¼šä¸‹è½½é¢„ç¼–è¯‘åŒ…ï¼ˆæ”¯æŒä»£ç†ï¼‰ ======================
echo -e "\033[34mã€æ­¥éª¤4/10ã€‘å°è¯•ä¸‹è½½llama.cpp_reranké¢„ç¼–è¯‘åŒ…...\033[0m"

# å®šä¹‰ä¸‹è½½å‡½æ•°ï¼ˆå¸¦è¶…æ—¶æ£€æµ‹ï¼‰
download_llama_package() {
    local download_url=$1
    # ä½¿ç”¨wgetä¸‹è½½ï¼Œ--timeout=10æ£€æµ‹è¿žæŽ¥è¶…æ—¶ï¼Œ--wait=1ç­‰å¾…ï¼Œ--show-progressæ˜¾ç¤ºè¿›åº¦
    if ! wget --timeout=10 --wait=1 --show-progress -O - "$download_url" 2> /tmp/wget_error.log | tar -zxf - -C "$LLAMA_ROOT_DIR/"; then
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿žæŽ¥è¶…æ—¶ï¼ˆ10ç§’æœªå¼€å§‹ï¼‰
        if grep -E "Timeout|timed out" /tmp/wget_error.log > /dev/null; then
            echo -e "\033[31mâŒ è¿žæŽ¥GitHubè¶…æ—¶ï¼ˆ10ç§’æœªå¼€å§‹ä¸‹è½½ï¼‰ï¼\033[0m"
            # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä½¿ç”¨ä»£ç†
            read -p "ðŸ“Œ æ˜¯å¦ä½¿ç”¨gh-proxy.orgä»£ç†ä¸‹è½½ï¼Ÿ(yes/YES/Y/y ç¡®è®¤ï¼Œå…¶ä»–å–æ¶ˆ)ï¼š" use_proxy
            if [[ "$use_proxy" =~ ^(yes|YES|Y|y)$ ]]; then
                echo -e "\033[33mðŸ”§ åˆ‡æ¢åˆ°ä»£ç†åœ°å€ä¸‹è½½...\033[0m"
                local proxy_url="${GH_PROXY_PREFIX}${LLAMA_DOWNLOAD_URL}"
                # é‡æ–°ä½¿ç”¨ä»£ç†åœ°å€ä¸‹è½½
                wget --show-progress -O - "$proxy_url" | tar -zxf - -C "$LLAMA_ROOT_DIR/"
                echo -e "\033[32mâœ… ä»£ç†ä¸‹è½½é¢„ç¼–è¯‘åŒ…å®Œæˆï¼\033[0m"
                return 0
            else
                echo -e "\033[31mâŒ ç”¨æˆ·å–æ¶ˆä»£ç†ä¸‹è½½ï¼Œè„šæœ¬é€€å‡ºï¼\033[0m"
                rm -f /tmp/wget_error.log
                exit 1
            fi
        else
            # å…¶ä»–é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶ä¸å­˜åœ¨ï¼‰
            echo -e "\033[31mâŒ ä¸‹è½½å¤±è´¥ï¼é”™è¯¯ä¿¡æ¯ï¼š\033[0m"
            cat /tmp/wget_error.log
            rm -f /tmp/wget_error.log
            exit 1
        fi
    else
        echo -e "\033[32mâœ… é¢„ç¼–è¯‘åŒ…ä¸‹è½½å¹¶è§£åŽ‹å®Œæˆï¼\033[0m"
        rm -f /tmp/wget_error.log
        return 0
    fi
}

# æ‰§è¡Œä¸‹è½½ï¼ˆå…ˆå°è¯•åŽŸå§‹åœ°å€ï¼‰
download_llama_package "$LLAMA_DOWNLOAD_URL"

# ====================== ç¬¬äº”æ­¥ï¼šä¸‹è½½Qwen3-Rerankeræ¨¡åž‹æ–‡ä»¶ ======================
echo -e "\033[34mã€æ­¥éª¤5/10ã€‘ä¸‹è½½Qwen3-Rerankeræ¨¡åž‹æ–‡ä»¶...\033[0m"
wget -q --show-progress -O "$MODEL_FILE_PATH" "$MODEL_DOWNLOAD_URL"
if [ -f "$MODEL_FILE_PATH" ]; then
    echo -e "\033[32mâœ… æ¨¡åž‹æ–‡ä»¶ä¸‹è½½å®Œæˆï¼š$MODEL_FILE_PATH\033[0m"
else
    echo -e "\033[31mâŒ æ¨¡åž‹æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä¸‹è½½åœ°å€ï¼\033[0m"
    exit 1
fi

# ====================== ç¬¬å…­æ­¥ï¼šåˆ›å»ºstart_llama.shå¯åŠ¨è„šæœ¬ï¼ˆä»…æ­¤å¤„é…ç½®CUDAè·¯å¾„ï¼‰ ======================
echo -e "\033[34mã€æ­¥éª¤6/10ã€‘åˆ›å»ºå¯åŠ¨è„šæœ¬start_llama.shï¼ˆé…ç½®CUDAåº“è·¯å¾„ï¼‰...\033[0m"
# æ‹¼æŽ¥llama-serverçš„å®Œæ•´è·¯å¾„
LLAMA_SERVER_PATH="$LLAMA_ROOT_DIR/llama-server"

# å†™å…¥å¯åŠ¨è„šæœ¬å†…å®¹ï¼ˆä»…æ­¤å¤„é…ç½®CUDAåº“è·¯å¾„ï¼Œæ— ä¸´æ—¶çŽ¯å¢ƒå˜é‡ï¼‰
cat > "$START_SCRIPT_PATH" << EOF
#!/bin/bash
# ä»…åœ¨å¯åŠ¨è„šæœ¬ä¸­é…ç½®CUDA 13åº“è·¯å¾„ï¼ˆæ°¸ä¹…ç”Ÿæ•ˆï¼‰
export LD_LIBRARY_PATH="$CUDA_LIB_DIR:\$LD_LIBRARY_PATH"
echo -e "\033[33mðŸ”§ å·²é…ç½®CUDAåº“è·¯å¾„ï¼šLD_LIBRARY_PATH=\$LD_LIBRARY_PATH\033[0m"

# æµ‹è¯•llama-serverå¯æ‰§è¡Œæ€§
if ! "\$LLAMA_SERVER_PATH" -h > /dev/null 2>&1; then
    echo -e "\033[31mâŒ llama-serveræ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥CUDAåº“æˆ–é¢„ç¼–è¯‘åŒ…ï¼\033[0m"
    exit 1
fi
echo -e "\033[32mâœ… llama-serverå¯æ‰§è¡Œæ€§æµ‹è¯•é€šè¿‡ï¼\033[0m"

# å¯åŠ¨llama-server
echo -e "\033[33mðŸš€ å¯åŠ¨llama-serverï¼ˆé‡æŽ’åºæ¨¡å¼ï¼‰...\033[0m"
"$LLAMA_SERVER_PATH" \
  --model "$MODEL_FILE_PATH" \
  --host 0.0.0.0 \
  --port 11435 \
  --no-webui \
  --rerank \
  --ctx-size 8192 \
  --n-gpu-layers 99 \
  --verbose
EOF

# æ·»åŠ å¯æ‰§è¡Œæƒé™
chmod +x "$START_SCRIPT_PATH"
echo -e "\033[32mâœ… å¯åŠ¨è„šæœ¬åˆ›å»ºå®Œæˆï¼š$START_SCRIPT_PATH\033[0m"

# ====================== ç¬¬ä¸ƒæ­¥ï¼šåˆ›å»ºsystemdå¼€æœºè‡ªå¯æœåŠ¡ ======================
echo -e "\033[34mã€æ­¥éª¤7/10ã€‘åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶...\033[0m"
cat > "$SERVICE_FILE_PATH" << EOF
[Unit]
Description=Llama Server for Rerank
After=network-online.target

[Service]
ExecStart=$START_SCRIPT_PATH
User=root
Group=root
Restart=always
RestartSec=3
# æœåŠ¡ä¸­æ— éœ€é‡å¤é…ç½®CUDAè·¯å¾„ï¼Œå¯åŠ¨è„šæœ¬å·²åŒ…å«

[Install]
WantedBy=multi-user.target
EOF

echo -e "\033[32mâœ… systemdæœåŠ¡æ–‡ä»¶åˆ›å»ºå®Œæˆï¼š$SERVICE_FILE_PATH\033[0m"

# ====================== ç¬¬å…«æ­¥ï¼šé‡æ–°åŠ è½½systemdå¹¶è®¾ç½®å¼€æœºè‡ªå¯ ======================
echo -e "\033[34mã€æ­¥éª¤8/10ã€‘é…ç½®å¼€æœºè‡ªå¯å¹¶å¯åŠ¨æœåŠ¡...\033[0m"
# é‡æ–°åŠ è½½systemdé…ç½®
systemctl daemon-reload
# è®¾ç½®å¼€æœºè‡ªå¯
systemctl enable llama-server
# å¯åŠ¨æœåŠ¡
systemctl start llama-server

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
if systemctl is-active --quiet llama-server; then
    echo -e "\033[32mâœ… llama-serveræœåŠ¡å¯åŠ¨æˆåŠŸï¼\033[0m"
else
    echo -e "\033[31mâŒ llama-serveræœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ‰§è¡Œ systemctl status llama-server æŸ¥çœ‹è¯¦æƒ…ï¼\033[0m"
fi

# ====================== ç¬¬ä¹æ­¥ï¼šæ‰§è¡Œå¯åŠ¨è„šæœ¬ï¼ˆåŒé‡ä¿éšœï¼‰ ======================
echo -e "\033[34mã€æ­¥éª¤9/10ã€‘æ‰§è¡Œå¯åŠ¨è„šæœ¬start_llama.sh...\033[0m"
# åŽå°æ‰§è¡Œå¯åŠ¨è„šæœ¬ï¼Œé¿å…é˜»å¡žç»ˆç«¯
bash "$START_SCRIPT_PATH" &
echo -e "\033[32mðŸŽ‰ æ‰€æœ‰æ“ä½œå®Œæˆï¼llama-serverå·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£11435\033[0m"

# ====================== ç¬¬åæ­¥ï¼šè¾“å‡ºå¸¸ç”¨å‘½ä»¤ ======================
echo -e "\033[34mã€æ­¥éª¤10/10ã€‘è¾“å‡ºå¸¸ç”¨è¿ç»´å‘½ä»¤...\033[0m"
echo -e "\033[33mðŸ“Œ å¸¸ç”¨å‘½ä»¤ï¼š\033[0m"
echo -e "  - æŸ¥çœ‹æœåŠ¡çŠ¶æ€ï¼šsystemctl status llama-server"
echo -e "  - é‡å¯æœåŠ¡ï¼šsystemctl restart llama-server"
echo -e "  - åœæ­¢æœåŠ¡ï¼šsystemctl stop llama-server"
echo -e "  - æ‰‹åŠ¨å¯åŠ¨ï¼šbash $START_SCRIPT_PATH"